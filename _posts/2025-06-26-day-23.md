---
layout: post
title: "Day 23 – Neural Networks Research"
date: 2025-06-26
author: Gracelyn Ruth Arunachalam
permalink: /day23.html
tags: ["Neural Networks", "Layers", "Input", "Output", "Brain", "Deep Learning"]

what_i_learned: |
  Unlike most days, today involved extensive research—particularly on neural networks. I began my exploration by watching several educational videos on YouTube that explained the fundamental concepts and theoretical underpinnings of deep learning.

  I learned that neural networks are composed of multiple layers of interconnected nodes, often compared to neurons in the human brain. Each node receives input from nodes in the previous layer, applies a set of learned weights and a mathematical function (usually a non-linear activation function), and passes the result to the next layer. This layered structure allows the network to learn complex patterns in data.

  At the output layer, the network typically produces a set of values (often normalized between 0 and 1), and the node with the highest value is considered the model’s predicted output. This architecture—especially in deep neural networks—enables the model to extract and recognize intricate features from the input data. Such networks are particularly effective in applications like image recognition, where identifying subtle patterns is crucial.

  In between my research, I was also able to condese my slides for our presentation into a concise and non-redundant list of my achievements, challenges and solutions.
  
blockers: |
  No blockers today!

reflection: |
  I was abe to improve my research and knowledge on neural networks. Even though the math was complex, I was able to grasp the general idea and benefits of neural networks. Despite the topic being useful in the field of image recognition, I obtained a general idea of what Machine Learning can truly do.
---
