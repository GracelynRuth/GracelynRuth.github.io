---
layout: post
title: "Day 20 â€“ Deploying Different ML Models and Assessing their Accuracy"
date: 2025-06-23
author: Gracelyn Ruth Arunachalam
permalink: /day20.html
tags: ["Random Forest", "Gradient Boosting", "Feature Importances", "R2", "Evaluation Metrics", "Correlation"]

what_i_learned: |
  A huge portion of our day today was dedicated to experimenting and analysing data from two ground stations, Padonia and Howard specifically. We began our day with a brief overview of the metrics library in python. I learned through that session that there are many ways to analyse whether or not the model you are testing is efficient and accurate. Some methods that we learned today were mean absolute error (MAE), mean squared error (MSE), r2 etc. The closer the value of r2 to 1 the more accurate the model and the lower the errors the better the model. Another ascpect of ML that we learned today was the importance of features on the outcome from ML models. We were each assigned an area in Maryland with AOD and PM2.5 data for us to clean, visualize and improve the r2 score of the model. 

  I worked on the data for Padonia and was able to clean it. Dr. Li also advised us to create graphs that show the correlation between time and PM2.5 and AOD. This was a bit tricky as I had to research about the various methods in the pandas library that would guide me to plot the right graph. One function that I had to implement was the datetime.time function that helped me obtain a series of all the different times of the day. Upon incoorporating all the functions that I need for this assignment, I was able to conclude that the WindSpeed and Date/Time have the greatest impact on the value of PM2.5. 
  
blockers: |
  Finding out the various functions in python that would help me plot the graphs was a little hard. Some functions also had return values that were not a python pandas series, therefore converting them to the right format also took a lot of trial and error.

reflection: |
   I was able to test my research skills by learning about various functions in python through websites and AI bots. Through our data visualization task, I was also able to use python's matplotlib.pyplot library to generate graphs that depicted trends and outliers in our data. One specific graph of the feature importances for our data, showed that correlation is not always causation. Overall, I believe working on this data has helped me think of different ways to visualize and analyse data.
---
